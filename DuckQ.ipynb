{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e66a47-88a7-4a67-b035-8713d170442a",
   "metadata": {},
   "source": [
    "# `DuckQ` - Deep Reinforcement Learning for Duck Game\n",
    "\n",
    "CPSC 480 Final Project. This notebook is not the full project. Please see final report for links to other parts.\n",
    "\n",
    "Students: Tiger Wang (tiger.wang@yale.edu), Garrek Chan (garrek.chan@yale.edu).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains the Python game environment declaration (which interfaces with our C# Duck Game server), the training loop, and the evaluation loop. Our game environment setup, which implements a custom `gym.Env`, is implemented from scratch. Our training and evaluation loop is based on the [PyTorch reinforcement learning tutorial](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#), and we make game-specific modifications.\n",
    "\n",
    "See final report for instructions on how to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0833d-e8cb-4736-bdf8-332f45285d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # We assume gym version >= 0.26\n",
    "from gym import spaces\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from gym.envs.registration import register\n",
    "import zmq\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "from gym.wrappers import FrameStack\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c5921-56cb-4643-a1ed-39a28bbc2b82",
   "metadata": {},
   "source": [
    "## Game Environment\n",
    "\n",
    "We implement a custom Gym environment from scratch. `DuckEnv` is inherited from `gym.Env` and implements required signatures such `reset()` and `step()`. The environment logic communicates with the ZeroMQ server that we have implemented in Duck Game for remote control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125e034-cec0-4cd6-827b-4380303f9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, render_mode=\"human\"):\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(84,84,3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.zmq_context = zmq.Context()\n",
    "        #  Socket to talk to server\n",
    "        self.socket = self.zmq_context.socket(zmq.REQ)\n",
    "        self.socket.connect(\"tcp://localhost:5556\")\n",
    "\n",
    "        self.step_count = 0\n",
    "\n",
    "    def _send(self, bytes):\n",
    "        self.socket.send(bytes)\n",
    "    \n",
    "    def _recv(self):\n",
    "        return self.socket.recv().decode()\n",
    "\n",
    "    def _recv_obs(self):\n",
    "        raw = self._recv()\n",
    "        kill_str, pixel_str = raw.split(';')\n",
    "        kill = (kill_str == 'True')\n",
    "        pixel_1d = np.fromstring(pixel_str, dtype=np.uint8, sep=' ')\n",
    "        pixel_rgb = np.reshape(pixel_1d, (84,84,3))\n",
    "        return kill, pixel_rgb\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Start new level\n",
    "        self._send(b\"reset\")\n",
    "        assert self._recv() == \"ack\"\n",
    "\n",
    "        # Pass for 150 frames to allow game to initialize\n",
    "        for _ in range(150):\n",
    "            self._send(b\"pass\")\n",
    "            assert self._recv() == \"ack\"\n",
    "\n",
    "        # Do a single step none to populate observation\n",
    "        self._send(b\"step none\")\n",
    "        kill, obs = self._recv_obs()\n",
    "\n",
    "        return obs, {\"kill\": kill}\n",
    "\n",
    "    def step(self, action):        \n",
    "        if action == 0:\n",
    "            # Do nothing\n",
    "            self._send(b\"step none\")\n",
    "        elif action == 1:\n",
    "            # Move left\n",
    "            self._send(b\"step left\")\n",
    "        elif action == 2:\n",
    "            # Move right\n",
    "            self._send(b\"step right\")\n",
    "        elif action == 3:\n",
    "            # Grab\n",
    "            self._send(b\"step grab\")\n",
    "        elif action == 4:\n",
    "            # Shoot\n",
    "            self._send(b\"step shoot\")\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        kill, obs = self._recv_obs()\n",
    "        if (obs.shape != (84,84,3)):\n",
    "            print(obs.shape)\n",
    "            print(obs)\n",
    "        reward = 1 if kill else 0\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count > self.spec.max_episode_steps:\n",
    "            return obs, reward, True, True, {\"kill\": kill, \"TimeLimit.truncated\": True}\n",
    "        else:\n",
    "            return obs, reward, kill, False, {\"kill\": kill, \"TimeLimit.truncated\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be257a92-1a9a-4a70-9a25-b10504878039",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='DuckGame',\n",
    "    entry_point='__main__:DuckEnv',\n",
    "    max_episode_steps=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9175d63-aef9-4d1e-a1a4-24a0de1508be",
   "metadata": {},
   "source": [
    "### Reused transformations\n",
    "\n",
    "We use a similar set of wrapper as [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#preprocess-environment) except we do not need to resize the observation and we slightly modified `GrayScaleObservation` for bug fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638871c4-e43d-4881-ad5a-36492fc84582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # print(observation.shape)\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        # print(observation.shape)\n",
    "        return observation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bed94c-98c0-4b86-b745-82b0f4dfd72c",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "`DuckQ` is a convolutional neural network that operates on 84x84 4-channel inputs. Each of the 4-channels corresponds to one of the stacked frames. It is based on an official PyTorch tutorial for reinforcement learning [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html). Duck Game input is also 84x84 grayscale, so we do not need to modify the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1ba3e-dd2d-49af-81a9-4592d617e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2e0c5-4a17-43c1-992e-318da506a4ad",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "We use logging logic based on [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html). This records each episode's average reward, loss, and length. It also periodically saves average reward line plots to the `checkpoints` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc9394-e44d-44f2-8422-9441828e9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa7ca9-603f-4722-bcaf-245c76d80584",
   "metadata": {},
   "source": [
    "## Agent Setup\n",
    "\n",
    "`DuckQ` (`Q` for Q-learning) is the agent that is trying to learn Duck Game. This object contains an instance of `DuckQNet`. It alternates between exploring (taking random actions to gather data) and exploiting (running inference using `DuckQNet`). The exploration rate (epsilon) is initially 100% and decreases by a factor of 0.9999972797. After 10,000 training episodes each with an average of 75 frames, the exploration rate decreases to around 13%. Our implementation is based on [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html) and [[2]](https://github.com/yfeng997/MadMario/blob/master/agent.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba07d-f1fb-4a84-b651-009b426c2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ:\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = 'cpu'\n",
    "\n",
    "        self.net = DuckQNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.9999972797 # 0.99998994971 # 0.9999975 # 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        if checkpoint:\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        self.save_every = 100000  # no. of experiences between saving Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "    \n",
    "        Inputs:\n",
    "        state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        ``action_idx`` (``int``): An integer representing which action agent will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            # print(\"state1\", state.shape, state)\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            # print(\"state2\", state.shape, state)\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            # print(\"state3\", state.shape, state)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d4d6c-6614-4c40-b65f-9fc153a1f4de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(10000, device=torch.device(\"cpu\"), scratch_dir='tmp'))\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        # print(\"cache\", state.shape, state)\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5dd60-7026-4c10-a922-1f1d1af3f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18774025-3713-4b28-8bf6-26fcd1307cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb0bda-7dba-47cf-85af-44656ff7d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"duckq_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"DuckQNet saved to {save_path} at step {self.curr_step}\")\n",
    "        \n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location='cpu')\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00670e-6811-477b-9913-d19393938b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afd2cb-4098-4ec4-b4b2-68ba589a68bd",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Environment Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ef01e-519e-4293-addc-147bc91c3752",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('DuckGame')\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b14fec-46c0-44ec-9541-e242a6ff3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1249ac-1180-42e4-940e-d22e672e2b4b",
   "metadata": {},
   "source": [
    "Below is a manual \"solution\" in which the duck takes 10 steps to the left, picks up the weapon, turns right to face the other duck, and shoot the weapon to eliminate the other duck. Ideally, we want our neural network to converge to a solution like this where `DuckQ` kills the other duck within 300 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55061dd4-ca4b-4dae-a1d5-b27f8cc88bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Manual Solution ==\n",
    "# for _ in range(10):\n",
    "#     env.step(1)\n",
    "\n",
    "# for _ in range(1):\n",
    "#     env.step(3)\n",
    "\n",
    "# for _ in range(1):\n",
    "#     env.step(2)\n",
    "\n",
    "# for _ in range(1):\n",
    "#     obs, reward, terminated, _, info = env.step(4)\n",
    "\n",
    "# for _ in range(10):\n",
    "#     obs, reward, terminated, _, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4694da6-9120-4a37-b85a-abcf853d9fbe",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We run training for 10,000 episodes and periodically save. Our training loop is based on [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a5cdf-ecee-4b78-b1a4-0cd817094810",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "duckq = DuckQ(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = duckq.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        duckq.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = duckq.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=duckq.exploration_rate, step=duckq.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979eaf4e-a55d-4ec5-8c93-5b636b033229",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "duckq.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549ab0f-5aa4-469d-a140-c763af900539",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate `DuckQ`. First, we load trained weights from a checkpoint file. Then we run the game for 100 episodes and compute the average reward and success rate. Our evaluation loop is based on [[3]](https://github.com/yfeng997/MadMario/blob/master/replay.py) and [[2]](https://github.com/yfeng997/MadMario/blob/master/agent.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2936a18-5e56-4467-9cfb-106b8ded79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('DuckGame')\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "env.reset()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "checkpoint = Path('models/duckq_net_1.chkpt')\n",
    "duckq = DuckQ(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
    "\n",
    "# Minimize exploration for evaluation \n",
    "duckq.exploration_rate = duckq.exploration_rate_min\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 100\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = duckq.act(state)\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        duckq.cache(state, next_state, action, reward, done)\n",
    "        logger.log_step(reward, None, None)\n",
    "        state = next_state\n",
    "        if info['kill']:\n",
    "            break\n",
    "        if done:\n",
    "            break\n",
    "    logger.log_episode()\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=duckq.exploration_rate,\n",
    "            step=duckq.curr_step\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
