{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e66a47-88a7-4a67-b035-8713d170442a",
   "metadata": {},
   "source": [
    "# `DuckQ` - Deep Reinforcement Learning for Duck Game\n",
    "\n",
    "CPSC 480 Final Project. This notebook is not the full project. Please see final report for links to other parts.\n",
    "\n",
    "Students: Tiger Wang (tiger.wang@yale.edu), Garrek Chan (garrek.chan@yale.edu).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains the Python game environment declaration (which interfaces with our C# Duck Game server), the training loop, and the evaluation loop. Our game environment setup, which implements a custom `gym.Env`, is implemented from scratch. Our training and evaluation loop is based on the [PyTorch reinforcement learning tutorial](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#), and we make game-specific modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b0833d-e8cb-4736-bdf8-332f45285d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # We assume gym version >= 0.26\n",
    "from gym import spaces\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from gym.envs.registration import register\n",
    "import zmq\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "from gym.wrappers import FrameStack\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c5921-56cb-4643-a1ed-39a28bbc2b82",
   "metadata": {},
   "source": [
    "## Game Environment\n",
    "\n",
    "We implement a custom Gym environment from scratch. `DuckEnv` is inherited from `gym.Env` and implements required signatures such `reset()` and `step()`. The environment logic communicates with the ZeroMQ server that we have implemented in Duck Game for remote control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a125e034-cec0-4cd6-827b-4380303f9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, render_mode=\"human\"):\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(84,84,3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.zmq_context = zmq.Context()\n",
    "        #  Socket to talk to server\n",
    "        self.socket = self.zmq_context.socket(zmq.REQ)\n",
    "        self.socket.connect(\"tcp://localhost:5556\")\n",
    "\n",
    "        self.step_count = 0\n",
    "\n",
    "    def _send(self, bytes):\n",
    "        self.socket.send(bytes)\n",
    "    \n",
    "    def _recv(self):\n",
    "        return self.socket.recv().decode()\n",
    "\n",
    "    def _recv_obs(self):\n",
    "        raw = self._recv()\n",
    "        kill_str, pixel_str = raw.split(';')\n",
    "        kill = (kill_str == 'True')\n",
    "        pixel_1d = np.fromstring(pixel_str, dtype=np.uint8, sep=' ')\n",
    "        pixel_rgb = np.reshape(pixel_1d, (84,84,3))\n",
    "        return kill, pixel_rgb\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Start new level\n",
    "        self._send(b\"reset\")\n",
    "        assert self._recv() == \"ack\"\n",
    "\n",
    "        # Pass for 150 frames to allow game to initialize\n",
    "        for _ in range(150):\n",
    "            self._send(b\"pass\")\n",
    "            assert self._recv() == \"ack\"\n",
    "\n",
    "        # Do a single step none to populate observation\n",
    "        self._send(b\"step none\")\n",
    "        kill, obs = self._recv_obs()\n",
    "\n",
    "        return obs, {\"kill\": kill}\n",
    "\n",
    "    def step(self, action):        \n",
    "        if action == 0:\n",
    "            # Do nothing\n",
    "            self._send(b\"step none\")\n",
    "        elif action == 1:\n",
    "            # Move left\n",
    "            self._send(b\"step left\")\n",
    "        elif action == 2:\n",
    "            # Move right\n",
    "            self._send(b\"step right\")\n",
    "        elif action == 3:\n",
    "            # Grab\n",
    "            self._send(b\"step grab\")\n",
    "        elif action == 4:\n",
    "            # Shoot\n",
    "            self._send(b\"step shoot\")\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        kill, obs = self._recv_obs()\n",
    "        if (obs.shape != (84,84,3)):\n",
    "            print(obs.shape)\n",
    "            print(obs)\n",
    "        reward = 1 if kill else 0\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count > self.spec.max_episode_steps:\n",
    "            return obs, reward, True, True, {\"kill\": kill, \"TimeLimit.truncated\": True}\n",
    "        else:\n",
    "            return obs, reward, kill, False, {\"kill\": kill, \"TimeLimit.truncated\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be257a92-1a9a-4a70-9a25-b10504878039",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='DuckGame',\n",
    "    entry_point='__main__:DuckEnv',\n",
    "    max_episode_steps=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9175d63-aef9-4d1e-a1a4-24a0de1508be",
   "metadata": {},
   "source": [
    "### Reused transformations\n",
    "\n",
    "We use a similar set of wrapper as [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#preprocess-environment) except we do not need to resize the observation and we slightly modified `GrayScaleObservation` for bug fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638871c4-e43d-4881-ad5a-36492fc84582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # print(observation.shape)\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        # print(observation.shape)\n",
    "        return observation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bed94c-98c0-4b86-b745-82b0f4dfd72c",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "`DuckQ` is a convolutional neural network that operates on 84x84 4-channel inputs. Each of the 4-channels corresponds to one of the stacked frames. It is based on [[1]]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html). Duck Game input is also 84x84 grayscale, so we do not need to modify the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c1ba3e-dd2d-49af-81a9-4592d617e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2e0c5-4a17-43c1-992e-318da506a4ad",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "We use logging logic based on [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html). This records each episode's average reward, loss, and length. It also periodically saves average reward line plots to the `checkpoints` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedc9394-e44d-44f2-8422-9441828e9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa7ca9-603f-4722-bcaf-245c76d80584",
   "metadata": {},
   "source": [
    "## Agent Setup\n",
    "\n",
    "`DuckQ` (`Q` for Q-learning) is the agent that is trying to learn Duck Game. This object contains an instance of `DuckQNet`. It alternates between exploring (taking random actions to gather data) and exploiting (running inference using `DuckQNet`). The exploration rate (epsilon) is initially 100% and decreases by a factor of 0.9999972797. After 10,000 training episodes each with an average of 75 frames, the exploration rate decreases to around 13%. Our implementation is based on [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html) and [[2]](https://github.com/yfeng997/MadMario/blob/master/agent.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39dba07d-f1fb-4a84-b651-009b426c2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ:\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = 'cpu'\n",
    "\n",
    "        self.net = DuckQNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.9999972797 # 0.99998994971 # 0.9999975 # 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        if checkpoint:\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        self.save_every = 100000  # no. of experiences between saving Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "    \n",
    "        Inputs:\n",
    "        state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        ``action_idx`` (``int``): An integer representing which action agent will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            # print(\"state1\", state.shape, state)\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            # print(\"state2\", state.shape, state)\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            # print(\"state3\", state.shape, state)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8d4d6c-6614-4c40-b65f-9fc153a1f4de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(10000, device=torch.device(\"cpu\"), scratch_dir='tmp'))\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        # print(\"cache\", state.shape, state)\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf5dd60-7026-4c10-a922-1f1d1af3f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18774025-3713-4b28-8bf6-26fcd1307cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddb0bda-7dba-47cf-85af-44656ff7d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"duckq_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"DuckQNet saved to {save_path} at step {self.curr_step}\")\n",
    "        \n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location='cpu')\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b00670e-6811-477b-9913-d19393938b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckQ(DuckQ):\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afd2cb-4098-4ec4-b4b2-68ba589a68bd",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Environment Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977ef01e-519e-4293-addc-147bc91c3752",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('DuckGame')\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6b14fec-46c0-44ec-9541-e242a6ff3e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 84, 84),\n",
      " 0.0,\n",
      " False,\n",
      " {'kill': False, 'TimeLimit.truncated': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger J. Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1249ac-1180-42e4-940e-d22e672e2b4b",
   "metadata": {},
   "source": [
    "Below is a manual \"solution\" in which the duck takes 10 steps to the left, picks up the weapon, turns right to face the other duck, and shoot the weapon to eliminate the other duck. Ideally, we want our neural network to converge to a solution like this where `DuckQ` kills the other duck within 300 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55061dd4-ca4b-4dae-a1d5-b27f8cc88bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Manual Solution ==\n",
    "# for _ in range(10):\n",
    "#     env.step(1)\n",
    "\n",
    "# for _ in range(1):\n",
    "#     env.step(3)\n",
    "\n",
    "# for _ in range(1):\n",
    "#     env.step(2)\n",
    "\n",
    "# for _ in range(1):\n",
    "#     obs, reward, terminated, _, info = env.step(4)\n",
    "\n",
    "# for _ in range(10):\n",
    "#     obs, reward, terminated, _, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4694da6-9120-4a37-b85a-abcf853d9fbe",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We run training for 10,000 episodes and periodically save. Our training loop is based on [[1]](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a5cdf-ecee-4b78-b1a4-0cd817094810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n",
      "Episode 0 - Step 76 - Epsilon 0.9997932782886737 - Mean Reward 0.0 - Mean Length 76.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 3.234 - Time 2023-12-18T12:56:08\n",
      "Episode 20 - Step 1509 - Epsilon 0.9959034754591952 - Mean Reward 0.095 - Mean Length 71.857 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 60.21 - Time 2023-12-18T12:57:08\n",
      "Episode 40 - Step 2963 - Epsilon 0.9919721369306272 - Mean Reward 0.073 - Mean Length 72.268 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 62.477 - Time 2023-12-18T12:58:11\n",
      "Episode 60 - Step 4483 - Epsilon 0.9878789376475968 - Mean Reward 0.049 - Mean Length 73.492 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 66.832 - Time 2023-12-18T12:59:18\n",
      "Episode 80 - Step 6003 - Epsilon 0.983802628234499 - Mean Reward 0.037 - Mean Length 74.111 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 72.131 - Time 2023-12-18T13:00:30\n",
      "Episode 100 - Step 7489 - Epsilon 0.9798337599510891 - Mean Reward 0.04 - Mean Length 74.13 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 69.3 - Time 2023-12-18T13:01:39\n",
      "Episode 120 - Step 8987 - Epsilon 0.9758490471341662 - Mean Reward 0.03 - Mean Length 74.78 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 67.315 - Time 2023-12-18T13:02:46\n",
      "Episode 140 - Step 10507 - Epsilon 0.971822376957284 - Mean Reward 0.02 - Mean Length 75.44 - Mean Loss 0.097 - Mean Q Value -0.018 - Time Delta 80.99 - Time 2023-12-18T13:04:07\n",
      "Episode 160 - Step 11995 - Epsilon 0.9678965735867177 - Mean Reward 0.03 - Mean Length 75.12 - Mean Loss 0.262 - Mean Q Value 0.07 - Time Delta 104.242 - Time 2023-12-18T13:05:52\n",
      "Episode 180 - Step 13470 - Epsilon 0.9640207199873196 - Mean Reward 0.04 - Mean Length 74.67 - Mean Loss 0.375 - Mean Q Value 0.078 - Time Delta 109.682 - Time 2023-12-18T13:07:41\n",
      "Episode 200 - Step 14990 - Epsilon 0.9600428573307221 - Mean Reward 0.03 - Mean Length 75.01 - Mean Loss 0.523 - Mean Q Value 0.097 - Time Delta 111.321 - Time 2023-12-18T13:09:33\n",
      "Episode 220 - Step 16510 - Epsilon 0.9560814086276721 - Mean Reward 0.02 - Mean Length 75.23 - Mean Loss 0.681 - Mean Q Value 0.092 - Time Delta 111.335 - Time 2023-12-18T13:11:24\n",
      "Episode 240 - Step 17973 - Epsilon 0.9522839532906965 - Mean Reward 0.03 - Mean Length 74.66 - Mean Loss 0.667 - Mean Q Value 0.113 - Time Delta 111.879 - Time 2023-12-18T13:13:16\n",
      "Episode 260 - Step 19493 - Epsilon 0.9483545203462274 - Mean Reward 0.02 - Mean Length 74.98 - Mean Loss 0.626 - Mean Q Value 0.138 - Time Delta 115.545 - Time 2023-12-18T13:15:11\n",
      "Episode 280 - Step 21013 - Epsilon 0.9444413015185783 - Mean Reward 0.01 - Mean Length 75.43 - Mean Loss 0.537 - Mean Q Value 0.263 - Time Delta 117.358 - Time 2023-12-18T13:17:09\n",
      "Episode 300 - Step 22503 - Epsilon 0.9406209900136132 - Mean Reward 0.03 - Mean Length 75.13 - Mean Loss 0.389 - Mean Q Value 0.359 - Time Delta 115.421 - Time 2023-12-18T13:19:04\n",
      "Episode 320 - Step 23962 - Epsilon 0.9368951363437777 - Mean Reward 0.04 - Mean Length 74.52 - Mean Loss 0.231 - Mean Q Value 0.483 - Time Delta 113.135 - Time 2023-12-18T13:20:57\n",
      "Episode 340 - Step 25396 - Epsilon 0.9332475067519838 - Mean Reward 0.05 - Mean Length 74.23 - Mean Loss 0.149 - Mean Q Value 0.598 - Time Delta 109.319 - Time 2023-12-18T13:22:47\n",
      "Episode 360 - Step 26916 - Epsilon 0.9293966243699953 - Mean Reward 0.05 - Mean Length 74.23 - Mean Loss 0.026 - Mean Q Value 0.603 - Time Delta 116.861 - Time 2023-12-18T13:24:43\n",
      "Episode 380 - Step 28370 - Epsilon 0.9257278222744708 - Mean Reward 0.07 - Mean Length 73.57 - Mean Loss 0.002 - Mean Q Value 0.586 - Time Delta 111.126 - Time 2023-12-18T13:26:35\n",
      "Episode 400 - Step 29816 - Epsilon 0.9220935695884737 - Mean Reward 0.07 - Mean Length 73.13 - Mean Loss 0.002 - Mean Q Value 0.586 - Time Delta 112.971 - Time 2023-12-18T13:28:28\n",
      "Episode 420 - Step 31336 - Epsilon 0.9182887119746236 - Mean Reward 0.06 - Mean Length 73.74 - Mean Loss 0.002 - Mean Q Value 0.576 - Time Delta 120.243 - Time 2023-12-18T13:30:28\n",
      "Episode 440 - Step 32856 - Epsilon 0.9144995544393091 - Mean Reward 0.04 - Mean Length 74.6 - Mean Loss 0.002 - Mean Q Value 0.564 - Time Delta 119.585 - Time 2023-12-18T13:32:27\n",
      "Episode 460 - Step 34376 - Epsilon 0.9107260321989082 - Mean Reward 0.04 - Mean Length 74.6 - Mean Loss 0.002 - Mean Q Value 0.552 - Time Delta 117.953 - Time 2023-12-18T13:34:25\n",
      "Episode 480 - Step 35851 - Epsilon 0.9070791128210643 - Mean Reward 0.03 - Mean Length 74.81 - Mean Loss 0.002 - Mean Q Value 0.543 - Time Delta 114.421 - Time 2023-12-18T13:36:20\n",
      "Episode 500 - Step 37306 - Epsilon 0.9034959515298683 - Mean Reward 0.04 - Mean Length 74.9 - Mean Loss 0.002 - Mean Q Value 0.532 - Time Delta 113.479 - Time 2023-12-18T13:38:13\n",
      "Episode 520 - Step 38721 - Epsilon 0.9000248728227348 - Mean Reward 0.06 - Mean Length 73.85 - Mean Loss 0.002 - Mean Q Value 0.53 - Time Delta 111.981 - Time 2023-12-18T13:40:05\n",
      "Episode 540 - Step 40241 - Epsilon 0.896311077820844 - Mean Reward 0.06 - Mean Length 73.85 - Mean Loss 0.002 - Mean Q Value 0.528 - Time Delta 119.831 - Time 2023-12-18T13:42:05\n",
      "Episode 560 - Step 41648 - Epsilon 0.8928870333636474 - Mean Reward 0.1 - Mean Length 72.72 - Mean Loss 0.002 - Mean Q Value 0.516 - Time Delta 111.894 - Time 2023-12-18T13:43:57\n",
      "Episode 580 - Step 43054 - Epsilon 0.8894784889246855 - Mean Reward 0.12 - Mean Length 72.03 - Mean Loss 0.002 - Mean Q Value 0.505 - Time Delta 111.217 - Time 2023-12-18T13:45:48\n",
      "Episode 600 - Step 44574 - Epsilon 0.8858082117287907 - Mean Reward 0.09 - Mean Length 72.68 - Mean Loss 0.002 - Mean Q Value 0.494 - Time Delta 119.349 - Time 2023-12-18T13:47:47\n",
      "Episode 620 - Step 45974 - Epsilon 0.8824410932097466 - Mean Reward 0.09 - Mean Length 72.53 - Mean Loss 0.002 - Mean Q Value 0.482 - Time Delta 110.468 - Time 2023-12-18T13:49:38\n",
      "Episode 640 - Step 47476 - Epsilon 0.8788428864931568 - Mean Reward 0.1 - Mean Length 72.35 - Mean Loss 0.002 - Mean Q Value 0.472 - Time Delta 118.211 - Time 2023-12-18T13:51:36\n",
      "Episode 660 - Step 48984 - Epsilon 0.8752450659702801 - Mean Reward 0.07 - Mean Length 73.36 - Mean Loss 0.002 - Mean Q Value 0.473 - Time Delta 120.193 - Time 2023-12-18T13:53:36\n",
      "Episode 680 - Step 50504 - Epsilon 0.8716335205012801 - Mean Reward 0.04 - Mean Length 74.5 - Mean Loss 0.002 - Mean Q Value 0.469 - Time Delta 119.72 - Time 2023-12-18T13:55:36\n",
      "Episode 700 - Step 52024 - Epsilon 0.8680368774421084 - Mean Reward 0.04 - Mean Length 74.5 - Mean Loss 0.002 - Mean Q Value 0.461 - Time Delta 119.526 - Time 2023-12-18T13:57:36\n",
      "Episode 720 - Step 53494 - Epsilon 0.8645726623142054 - Mean Reward 0.04 - Mean Length 75.2 - Mean Loss 0.001 - Mean Q Value 0.453 - Time Delta 116.427 - Time 2023-12-18T13:59:32\n",
      "Episode 740 - Step 55014 - Epsilon 0.8610051546496618 - Mean Reward 0.03 - Mean Length 75.38 - Mean Loss 0.001 - Mean Q Value 0.445 - Time Delta 119.882 - Time 2023-12-18T14:01:32\n",
      "Episode 760 - Step 56534 - Epsilon 0.857452367680661 - Mean Reward 0.02 - Mean Length 75.5 - Mean Loss 0.001 - Mean Q Value 0.436 - Time Delta 118.968 - Time 2023-12-18T14:03:31\n",
      "Episode 780 - Step 58054 - Epsilon 0.8539142406648315 - Mean Reward 0.02 - Mean Length 75.5 - Mean Loss 0.001 - Mean Q Value 0.431 - Time Delta 120.37 - Time 2023-12-18T14:05:31\n",
      "Episode 800 - Step 59561 - Epsilon 0.8504207868152479 - Mean Reward 0.03 - Mean Length 75.37 - Mean Loss 0.001 - Mean Q Value 0.43 - Time Delta 118.75 - Time 2023-12-18T14:07:30\n",
      "Episode 820 - Step 61013 - Epsilon 0.8470683511503229 - Mean Reward 0.02 - Mean Length 75.19 - Mean Loss 0.001 - Mean Q Value 0.421 - Time Delta 114.985 - Time 2023-12-18T14:09:25\n",
      "Episode 840 - Step 62533 - Epsilon 0.8435730719599874 - Mean Reward 0.02 - Mean Length 75.19 - Mean Loss 0.001 - Mean Q Value 0.408 - Time Delta 119.851 - Time 2023-12-18T14:11:25\n",
      "Episode 860 - Step 64053 - Epsilon 0.8400922154270458 - Mean Reward 0.02 - Mean Length 75.19 - Mean Loss 0.001 - Mean Q Value 0.395 - Time Delta 120.226 - Time 2023-12-18T14:13:25\n",
      "Episode 880 - Step 65534 - Epsilon 0.8367144859132483 - Mean Reward 0.03 - Mean Length 74.8 - Mean Loss 0.001 - Mean Q Value 0.382 - Time Delta 117.109 - Time 2023-12-18T14:15:22\n",
      "Episode 900 - Step 67023 - Epsilon 0.8333322015881705 - Mean Reward 0.03 - Mean Length 74.62 - Mean Loss 0.001 - Mean Q Value 0.371 - Time Delta 117.064 - Time 2023-12-18T14:17:19\n",
      "Episode 920 - Step 68451 - Epsilon 0.8301013239677147 - Mean Reward 0.05 - Mean Length 74.38 - Mean Loss 0.001 - Mean Q Value 0.367 - Time Delta 114.007 - Time 2023-12-18T14:19:13\n",
      "Episode 940 - Step 69971 - Epsilon 0.8266760562433391 - Mean Reward 0.05 - Mean Length 74.38 - Mean Loss 0.001 - Mean Q Value 0.367 - Time Delta 118.704 - Time 2023-12-18T14:21:12\n",
      "Episode 960 - Step 71373 - Epsilon 0.8235292293172448 - Mean Reward 0.07 - Mean Length 73.2 - Mean Loss 0.001 - Mean Q Value 0.359 - Time Delta 111.173 - Time 2023-12-18T14:23:03\n",
      "Episode 980 - Step 72833 - Epsilon 0.8202649514519024 - Mean Reward 0.07 - Mean Length 72.99 - Mean Loss 0.001 - Mean Q Value 0.351 - Time Delta 115.824 - Time 2023-12-18T14:24:59\n",
      "Episode 1000 - Step 74264 - Epsilon 0.8170780681947967 - Mean Reward 0.08 - Mean Length 72.41 - Mean Loss 0.001 - Mean Q Value 0.343 - Time Delta 112.595 - Time 2023-12-18T14:26:52\n",
      "Episode 1020 - Step 75739 - Epsilon 0.813806153552238 - Mean Reward 0.06 - Mean Length 72.88 - Mean Loss 0.001 - Mean Q Value 0.335 - Time Delta 115.9 - Time 2023-12-18T14:28:47\n",
      "Episode 1040 - Step 77158 - Epsilon 0.8106708267552217 - Mean Reward 0.08 - Mean Length 71.87 - Mean Loss 0.001 - Mean Q Value 0.326 - Time Delta 112.87 - Time 2023-12-18T14:30:40\n",
      "Episode 1060 - Step 78648 - Epsilon 0.8073916233984747 - Mean Reward 0.07 - Mean Length 72.75 - Mean Loss 0.001 - Mean Q Value 0.326 - Time Delta 118.654 - Time 2023-12-18T14:32:39\n",
      "Episode 1080 - Step 80168 - Epsilon 0.8040600632759837 - Mean Reward 0.06 - Mean Length 73.35 - Mean Loss 0.001 - Mean Q Value 0.325 - Time Delta 119.307 - Time 2023-12-18T14:34:38\n",
      "Episode 1100 - Step 81688 - Epsilon 0.800742250252829 - Mean Reward 0.04 - Mean Length 74.24 - Mean Loss 0.001 - Mean Q Value 0.319 - Time Delta 119.124 - Time 2023-12-18T14:36:37\n",
      "Episode 1120 - Step 83208 - Epsilon 0.7974381276040106 - Mean Reward 0.03 - Mean Length 74.69 - Mean Loss 0.001 - Mean Q Value 0.314 - Time Delta 119.965 - Time 2023-12-18T14:38:37\n",
      "Episode 1140 - Step 84728 - Epsilon 0.7941476388386002 - Mean Reward 0.01 - Mean Length 75.7 - Mean Loss 0.001 - Mean Q Value 0.307 - Time Delta 119.654 - Time 2023-12-18T14:40:37\n",
      "Episode 1160 - Step 86248 - Epsilon 0.7908707276987632 - Mean Reward 0.0 - Mean Length 76.0 - Mean Loss 0.001 - Mean Q Value 0.301 - Time Delta 120.375 - Time 2023-12-18T14:42:37\n",
      "Episode 1180 - Step 87678 - Epsilon 0.787800189569552 - Mean Reward 0.02 - Mean Length 75.1 - Mean Loss 0.001 - Mean Q Value 0.295 - Time Delta 114.608 - Time 2023-12-18T14:44:32\n",
      "Episode 1200 - Step 89198 - Epsilon 0.7845494700673931 - Mean Reward 0.02 - Mean Length 75.1 - Mean Loss 0.001 - Mean Q Value 0.295 - Time Delta 119.814 - Time 2023-12-18T14:46:32\n",
      "Episode 1220 - Step 90718 - Epsilon 0.7813121640899094 - Mean Reward 0.02 - Mean Length 75.1 - Mean Loss 0.001 - Mean Q Value 0.292 - Time Delta 119.188 - Time 2023-12-18T14:48:31\n",
      "Episode 1240 - Step 92149 - Epsilon 0.7782766197211022 - Mean Reward 0.04 - Mean Length 74.21 - Mean Loss 0.001 - Mean Q Value 0.286 - Time Delta 113.861 - Time 2023-12-18T14:50:25\n",
      "Episode 1260 - Step 93648 - Epsilon 0.7751094754938326 - Mean Reward 0.05 - Mean Length 74.0 - Mean Loss 0.001 - Mean Q Value 0.279 - Time Delta 121.73 - Time 2023-12-18T14:52:27\n",
      "Episode 1280 - Step 95076 - Epsilon 0.7721043307832317 - Mean Reward 0.05 - Mean Length 73.98 - Mean Loss 0.001 - Mean Q Value 0.272 - Time Delta 123.812 - Time 2023-12-18T14:54:30\n",
      "Episode 1300 - Step 96556 - Epsilon 0.769002049705772 - Mean Reward 0.06 - Mean Length 73.58 - Mean Loss 0.001 - Mean Q Value 0.266 - Time Delta 134.582 - Time 2023-12-18T14:56:45\n",
      "Episode 1320 - Step 97969 - Epsilon 0.7660518416088397 - Mean Reward 0.09 - Mean Length 72.51 - Mean Loss 0.001 - Mean Q Value 0.262 - Time Delta 119.322 - Time 2023-12-18T14:58:44\n",
      "Episode 1340 - Step 99489 - Epsilon 0.7628908628554026 - Mean Reward 0.07 - Mean Length 73.4 - Mean Loss 0.001 - Mean Q Value 0.262 - Time Delta 126.144 - Time 2023-12-18T15:00:50\n",
      "DuckQNet saved to checkpoints\\2023-12-18T12-56-05\\duckq_net_1.chkpt at step 100000\n",
      "Episode 1360 - Step 100966 - Epsilon 0.7598318019793692 - Mean Reward 0.07 - Mean Length 73.18 - Mean Loss 0.001 - Mean Q Value 0.259 - Time Delta 122.809 - Time 2023-12-18T15:02:53\n",
      "Episode 1380 - Step 102486 - Epsilon 0.7566964891300478 - Mean Reward 0.05 - Mean Length 74.1 - Mean Loss 0.001 - Mean Q Value 0.254 - Time Delta 132.432 - Time 2023-12-18T15:05:06\n",
      "Episode 1400 - Step 104006 - Epsilon 0.7535741136000627 - Mean Reward 0.04 - Mean Length 74.5 - Mean Loss 0.001 - Mean Q Value 0.249 - Time Delta 133.442 - Time 2023-12-18T15:07:19\n",
      "Episode 1420 - Step 105461 - Epsilon 0.7505973306979636 - Mean Reward 0.02 - Mean Length 74.92 - Mean Loss 0.001 - Mean Q Value 0.245 - Time Delta 121.346 - Time 2023-12-18T15:09:20\n",
      "Episode 1440 - Step 106951 - Epsilon 0.7475611275867197 - Mean Reward 0.03 - Mean Length 74.62 - Mean Loss 0.001 - Mean Q Value 0.24 - Time Delta 125.849 - Time 2023-12-18T15:11:26\n",
      "Episode 1460 - Step 108471 - Epsilon 0.7444764475261204 - Mean Reward 0.02 - Mean Length 75.05 - Mean Loss 0.001 - Mean Q Value 0.238 - Time Delta 128.734 - Time 2023-12-18T15:13:35\n",
      "Episode 1480 - Step 109991 - Epsilon 0.7414044958575202 - Mean Reward 0.02 - Mean Length 75.05 - Mean Loss 0.001 - Mean Q Value 0.239 - Time Delta 132.752 - Time 2023-12-18T15:15:48\n",
      "Episode 1500 - Step 111494 - Epsilon 0.7383793657439516 - Mean Reward 0.03 - Mean Length 74.88 - Mean Loss 0.0 - Mean Q Value 0.234 - Time Delta 126.619 - Time 2023-12-18T15:17:54\n",
      "Episode 1520 - Step 113014 - Epsilon 0.7353325726154469 - Mean Reward 0.02 - Mean Length 75.53 - Mean Loss 0.0 - Mean Q Value 0.229 - Time Delta 128.371 - Time 2023-12-18T15:20:03\n",
      "Episode 1540 - Step 114534 - Epsilon 0.7322983515451533 - Mean Reward 0.01 - Mean Length 75.83 - Mean Loss 0.0 - Mean Q Value 0.224 - Time Delta 126.55 - Time 2023-12-18T15:22:09\n",
      "Episode 1560 - Step 116038 - Epsilon 0.729308393010998 - Mean Reward 0.02 - Mean Length 75.67 - Mean Loss 0.0 - Mean Q Value 0.219 - Time Delta 124.884 - Time 2023-12-18T15:24:14\n",
      "Episode 1580 - Step 117558 - Epsilon 0.7262990296627325 - Mean Reward 0.02 - Mean Length 75.67 - Mean Loss 0.0 - Mean Q Value 0.214 - Time Delta 133.765 - Time 2023-12-18T15:26:28\n",
      "Episode 1600 - Step 119078 - Epsilon 0.723302083925243 - Mean Reward 0.01 - Mean Length 75.84 - Mean Loss 0.0 - Mean Q Value 0.214 - Time Delta 130.934 - Time 2023-12-18T15:28:39\n",
      "Episode 1620 - Step 120598 - Epsilon 0.7203175045594369 - Mean Reward 0.01 - Mean Length 75.84 - Mean Loss 0.0 - Mean Q Value 0.212 - Time Delta 137.098 - Time 2023-12-18T15:30:56\n",
      "Episode 1640 - Step 122091 - Epsilon 0.7173979301892287 - Mean Reward 0.03 - Mean Length 75.57 - Mean Loss 0.0 - Mean Q Value 0.207 - Time Delta 135.885 - Time 2023-12-18T15:33:12\n",
      "Episode 1660 - Step 123611 - Epsilon 0.7144377132797246 - Mean Reward 0.02 - Mean Length 75.73 - Mean Loss 0.0 - Mean Q Value 0.201 - Time Delta 138.245 - Time 2023-12-18T15:35:30\n",
      "Episode 1680 - Step 125131 - Epsilon 0.7114897111868285 - Mean Reward 0.02 - Mean Length 75.73 - Mean Loss 0.0 - Mean Q Value 0.196 - Time Delta 131.292 - Time 2023-12-18T15:37:41\n",
      "Episode 1700 - Step 126591 - Epsilon 0.7086695318501816 - Mean Reward 0.03 - Mean Length 75.13 - Mean Loss 0.0 - Mean Q Value 0.191 - Time Delta 123.499 - Time 2023-12-18T15:39:45\n",
      "Episode 1720 - Step 128061 - Epsilon 0.7058413297579476 - Mean Reward 0.04 - Mean Length 74.63 - Mean Loss 0.0 - Mean Q Value 0.187 - Time Delta 129.517 - Time 2023-12-18T15:41:54\n",
      "Episode 1740 - Step 129581 - Epsilon 0.7029287991360288 - Mean Reward 0.02 - Mean Length 74.9 - Mean Loss 0.0 - Mean Q Value 0.188 - Time Delta 136.387 - Time 2023-12-18T15:44:11\n",
      "Episode 1760 - Step 131101 - Epsilon 0.7000282865616039 - Mean Reward 0.02 - Mean Length 74.9 - Mean Loss 0.0 - Mean Q Value 0.185 - Time Delta 132.164 - Time 2023-12-18T15:46:23\n",
      "Episode 1780 - Step 132621 - Epsilon 0.6971397424443033 - Mean Reward 0.02 - Mean Length 74.9 - Mean Loss 0.0 - Mean Q Value 0.182 - Time Delta 137.469 - Time 2023-12-18T15:48:40\n",
      "Episode 1800 - Step 134092 - Epsilon 0.694355665286156 - Mean Reward 0.02 - Mean Length 75.01 - Mean Loss 0.0 - Mean Q Value 0.178 - Time Delta 133.488 - Time 2023-12-18T15:50:54\n",
      "Episode 1820 - Step 135612 - Epsilon 0.6914905282470134 - Mean Reward 0.01 - Mean Length 75.51 - Mean Loss 0.0 - Mean Q Value 0.175 - Time Delta 133.032 - Time 2023-12-18T15:53:07\n",
      "Episode 1840 - Step 137132 - Epsilon 0.6886372136940443 - Mean Reward 0.01 - Mean Length 75.51 - Mean Loss 0.0 - Mean Q Value 0.172 - Time Delta 132.269 - Time 2023-12-18T15:55:19\n",
      "Episode 1860 - Step 138590 - Epsilon 0.6859113480937741 - Mean Reward 0.02 - Mean Length 74.89 - Mean Loss 0.0 - Mean Q Value 0.171 - Time Delta 124.79 - Time 2023-12-18T15:57:24\n",
      "Episode 1880 - Step 140052 - Epsilon 0.6831888384526631 - Mean Reward 0.04 - Mean Length 74.31 - Mean Loss 0.0 - Mean Q Value 0.171 - Time Delta 120.939 - Time 2023-12-18T15:59:25\n",
      "Episode 1900 - Step 141572 - Epsilon 0.6803697793686211 - Mean Reward 0.03 - Mean Length 74.8 - Mean Loss 0.0 - Mean Q Value 0.167 - Time Delta 132.985 - Time 2023-12-18T16:01:38\n",
      "Episode 1920 - Step 143092 - Epsilon 0.6775623526381442 - Mean Reward 0.03 - Mean Length 74.8 - Mean Loss 0.0 - Mean Q Value 0.163 - Time Delta 130.671 - Time 2023-12-18T16:03:49\n",
      "Episode 1940 - Step 144612 - Epsilon 0.6747665102623608 - Mean Reward 0.03 - Mean Length 74.8 - Mean Loss 0.0 - Mean Q Value 0.16 - Time Delta 133.984 - Time 2023-12-18T16:06:03\n",
      "Episode 1960 - Step 146112 - Epsilon 0.6720187653485605 - Mean Reward 0.03 - Mean Length 75.22 - Mean Loss 0.0 - Mean Q Value 0.156 - Time Delta 136.109 - Time 2023-12-18T16:08:19\n",
      "Episode 1980 - Step 147632 - Epsilon 0.6692457976147889 - Mean Reward 0.01 - Mean Length 75.8 - Mean Loss 0.0 - Mean Q Value 0.153 - Time Delta 135.206 - Time 2023-12-18T16:10:34\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "duckq = DuckQ(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = duckq.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        duckq.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = duckq.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=duckq.exploration_rate, step=duckq.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979eaf4e-a55d-4ec5-8c93-5b636b033229",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "duckq.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549ab0f-5aa4-469d-a140-c763af900539",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate `DuckQ`. First, we load trained weights from a checkpoint file. Then we run the game for 100 episodes and compute the average reward and success rate. Our evaluation loop is based on [[3]](https://github.com/yfeng997/MadMario/blob/master/replay.py) and [[2]](https://github.com/yfeng997/MadMario/blob/master/agent.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2936a18-5e56-4467-9cfb-106b8ded79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model at checkpoints\\2023-12-18T02-48-37\\duckq_net_2.chkpt with exploration rate 0.8179068615153847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger J. Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 76 - Epsilon 0.1 - Mean Reward 0.0 - Mean Length 76.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.184 - Time 2023-12-18T03:45:31\n",
      "Episode 20 - Step 1596 - Epsilon 0.1 - Mean Reward 0.0 - Mean Length 76.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 102.572 - Time 2023-12-18T03:47:13\n",
      "Episode 40 - Step 3116 - Epsilon 0.1 - Mean Reward 0.0 - Mean Length 76.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 104.8 - Time 2023-12-18T03:48:58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m duckq\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m---> 23\u001b[0m     next_state, reward, done, trunc, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     duckq\u001b[38;5;241m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[0;32m     25\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_step(reward, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\frame_stack.py:173\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mSkipFrame.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     10\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Accumulate reward and repeat the same action\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     obs, reward, done, trunk, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 77\u001b[0m, in \u001b[0;36mDuckEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m kill, obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (obs\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m84\u001b[39m,\u001b[38;5;241m84\u001b[39m,\u001b[38;5;241m3\u001b[39m)):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(obs\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mDuckEnv._recv_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     kill_str, pixel_str \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m     kill \u001b[38;5;241m=\u001b[39m (kill_str \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mDuckEnv._recv\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode()\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:805\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:841\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:194\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('DuckGame')\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "env.reset()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "# checkpoints/2023-12-18T02-48-37/duckq_net_2.chkpt - knows how to walk left and pick up gun, but doesn't know how to shoot\n",
    "checkpoint = Path('checkpoints/2023-12-18T02-48-37/duckq_net_2.chkpt')\n",
    "duckq = DuckQ(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
    "\n",
    "# Minimize exploration for evaluation \n",
    "duckq.exploration_rate = duckq.exploration_rate_min\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 100\n",
    "count_success = 0\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = duckq.act(state)\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        duckq.cache(state, next_state, action, reward, done)\n",
    "        logger.log_step(reward, None, None)\n",
    "        state = next_state\n",
    "        if info['kill']:\n",
    "            count_success += 1\n",
    "            break\n",
    "        if done:\n",
    "            break\n",
    "    logger.log_episode()\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=duckq.exploration_rate,\n",
    "            step=duckq.curr_step\n",
    "        )\n",
    "print(\"Number of successful kills:\", count_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2586915-7b9c-474a-bb12-06712f89179a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
